# 使用随机量化量化深度神经网络

[原文链接](https://arxiv.org/abs/1708.01001)

作者：

* Yinpeng Dong (dyp17@mails.tsinghua.edu.cn) 计算机科学与技术系，清华大学，北京，中国
* Renkun Ni (rn9zm@virginia.edu) 美国弗吉尼亚大学
* Jianguo Li (jianguo.li@intel.com) 英特尔实验室的中国北京，中国
* Yurong Chen(yurong.chen@intel.com) 英特尔实验室的中国北京，中国
* Jun Zhu (dcszj@mail.tsinghua.edu.cn) 计算机科学与技术系，清华大学，北京，中国
* Hang Su (suhangss@mail.tsinghua.edu.cn) 计算机科学与技术系，清华大学，北京，中国

## 摘要

由于嵌入式应用对存储需求和计算能力较为苛刻，使得使用较少位数表示深度神经网络势在必行。不过，这样会使得精度有所损失。本文的目的是使用随机量化(SQ)算法来将DNN网络使用低位数表示。我们使用这种量化方式是基于以下的观察。现有的训练算法在将每一次迭代中都近似地将实数值/滤波器汇总在一起，并使用低位表示。有些数值/滤波器的量化损失比较小，但有些却十分大，这会导致训练时梯度指向不合适的方向，这回导致精度显著下降。另外，SQ量化将一部分数值/滤波器量化为低位表示，这里量化误差与随机概率成反比(其他未量化的值保持全精度)。每次迭代中，量化和全精度的部分会根据相关的梯度进行更新。SQ率会随着整个网络的量化而增加。这个过程中会对量化的误差进行补偿，并且使低位表示的DNN获取较好的精度。我们用实验展示了SQ算法，可以在各种数据集和网络结构下保证(并提高)不同位数表示的DNN网络的精度。



## 1. 算法介绍

深度神经网络(DNN)在大多数机器视觉任务上可以具有显著的性能提升，例如图像分类，物体检测和语法分割。DNN通常具有几十层或上百层，那其中的参数的数量几乎是天文数字。因此，使用DNN的系统通常要考虑自身的存储和计算能力。这就阻碍了DNN应用在资源比较缺乏的系统上的部署，尤其是物联网领域的嵌入式设备。

很多方法为了减少模型参数的尺寸或计算的复杂度，导致DNN网络过度冗余。其中，低位宽深度神经网络旨在使用少量的位数来表示模型权重，甚至是激活输出，由于其对模型尺寸和计算量都很有效，所以吸引了很多注意。在二值链接(BNN)中，权重只有两个值，+1和-1，并且使用加法和减法来代替乘法，从而加速计算。在《Xnor-net:Imagenet classification using binary convolutional neural networks》这篇论文中，作者将二值权重网络(BWN)的每个滤波器加上特定的缩放因子，并且讲这种方法扩展到XNOR-Net(将权重和激活数据都进行二值化)。另外，三值权重网络(TWN)增加了0，元素由+1,0和-1表示，使用两个bit。不过，低位深度神经网络也被不可忽略的精度下降所困扰，特别是特别大的模型(比如：ResNet)。我们认为由于模型的尺寸的原因，导致这些放在每一次训练迭代是对DNN的权重进行低位量化。并不是所有元素和滤波器的量化误差都特别小。大误差的元素和滤波器将会在训练时将梯度的方向带跑偏，并且会让模型收敛与相对较差的局部最小。

除了在训练时的低位量化方案，这里同样也有后量化的方案，也就是对预训练的全精度模型直接进行量化。这种方法有两个局限。首先，其对做分类的DNN网络较好，不过对于其他类型(比如：检测和语法分割)的网络就缺乏灵活性。其次，低位量化可作为对局部最小值的量化解决方案或正则约束。不过，将局部最小从全精度量化为低位宽，并做到权重无损，对于后量化方案(即使有微调)也是相对困难的。

本文中，我们尝试对DNN使用随机量化(SQ)算法对上述问题进行攻克。受随机深度和dropout的影响，SQ算法会在每次迭代时，随机选择DNN中的部分权重，并使用低位对其进行量化，不过会保存这部分全精度的数值。选择概率与量化误差成反比。我们会逐渐的增加SQ率，直到网络完全量化。

我们在不同方面对SQ算法进行了一个综合的调研。首先，我们对选择粒度的影响进行调研，将滤波器的每个通道看作为随机选择的量化结果，要好于将每个权重值进行量化的结果。这是因为滤波器中的每个通道中的权重会互相作用，从而正确表示滤波器的结构，因此当把权重独立看待时(滤波器通道中一些权重是量化过的，有些是全精度表示)，可能会带来结果的变形。其次，我们比较了轮盘(*roulette*)算法和一些具有选择性的算法进行了比较。轮盘算法选择量化的值/滤波器其概率与量化误差成反比。如果量化误差特别明显，我们就不能对相应的值/滤波器进行量化，因为量化的话可能带来不准确的梯度信息。轮盘算法比其他选择算法好的方面在于，它消除了寻找最佳初始分区的要求，并且由于随机算法的探索性，使得其有能力为解决方案提供更好的搜索空间。其三，我们对比了不同方法间计算量化与所形成量化误差比例。其四，我们设计了一种可行的方案用于计算SQ率。

SQ算法普遍适用与任意低位DNN网络，包括BWN和TWN，等等。实验表明SQ方法可以提升低位DNN网络的精度(二值或三值)，可使用任意网络结构(VGGNet, ResNet等)，以及任意数据集(CIFAR，ImageNet等)。例如，TWN使用SQ算法进行训练，在很多测试集上能与全精度模型相抗衡。

我们做了如下几件事：

1.  我们的使用随机量化(SQ)算法克服目前低位DNN训练算法所存在的精度损失问题。
2.  我们在不同场景对SQ进行综合调研，比如选择粒度，划分算法，量化概率的方法和更新SQ率的方案，这些对于在深度学习领域对其他随机算法进行研究的人员来说，可能会有较大的帮助。
3.  各种低位设置，网络架构和测试数据下，我们的方案都对性能有很大的提升。


## 2. 低位深度神经网络

本节我们先来简单的介绍几个典型的低位DNN网络，包括BNN，BWN和TWN。我们将DNN的第$$l$$ 层表示为$$\mathcal W_l = \{ W_1,...,W_i,...,W_m \}$$ ，这里的$$m$$表示为输出通道的数量。$$W_i \in\mathbb{R}^d$$ 用来表示滤波器第$$i$$个通道的权重向量，这里卷积层中的$$d =n \times w \times h$$，全连接层中的$$d=n$$(这里的$$n, w,h$$表示*输入通道数*，*卷积核宽度*和*卷积核高度*)。
